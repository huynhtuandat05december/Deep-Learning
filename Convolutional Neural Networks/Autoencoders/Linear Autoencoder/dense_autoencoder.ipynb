{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Autoencoder for Anomaly Detection\n",
    "\n",
    "Anomaly detection is the task of finding anomalous data elements in a dataset. An anomaly is a data element that is an outlier with respect to the rest of the dataset.\n",
    "\n",
    "We are going to train an autoencoder on the MNIST dataset (that only contains numbers), and then we will look into anomalies within the MNIST dataset (i.e., images within MNIST that are somehow different than the rest of the dataset).\n",
    "\n",
    "Even though MNIST is a labeled dataset, we are going to disregard the labels for educational purposes and consider it as an unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python-headless==4.5.3.56\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
      "Collecting bokeh==2.1.1\n",
      "  Downloading bokeh-2.1.1.tar.gz (19.3 MB)\n",
      "Collecting torchvision==0.12.0\n",
      "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting tqdm==4.63.0\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Collecting ipywidgets==7.6.5\n",
      "  Downloading ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
      "Collecting livelossplot==0.5.4\n",
      "  Downloading livelossplot-0.5.4-py3-none-any.whl (22 kB)\n",
      "Collecting pytest==7.1.1\n",
      "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting seaborn==0.11.2\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Collecting jupyterlab-widgets>=1.0.0; python_version >= \"3.6\"\n",
      "  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.2.0-py3-none-any.whl (17 kB)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: bokeh\n",
      "  Building wheel for bokeh (setup.py): started\n",
      "  Building wheel for bokeh (setup.py): finished with status 'done'\n",
      "  Created wheel for bokeh: filename=bokeh-2.1.1-py3-none-any.whl size=9257186 sha256=9c22792570394da597686d6f4e62200473a50e17f6da3d7f7d8e8d629cdad5c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/55/ff/f3d7554e69382d31cf7ad857cf518af9b923134fca7d925187\n",
      "Successfully built bokeh\n",
      "Installing collected packages: opencv-python-headless, bokeh, torchvision, tqdm, jupyterlab-widgets, widgetsnbextension, ipywidgets, livelossplot, iniconfig, py, pluggy, tomli, pytest, pandas, seaborn\n",
      "\u001b[33m  WARNING: The script bokeh is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed bokeh-2.1.1 iniconfig-2.0.0 ipywidgets-7.6.5 jupyterlab-widgets-3.0.8 livelossplot-0.5.4 opencv-python-headless-4.5.3.56 pandas-1.3.5 pluggy-1.2.0 py-1.11.0 pytest-7.1.1 seaborn-0.11.2 tomli-2.0.1 torchvision-0.12.0 tqdm-4.63.0 widgetsnbextension-3.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt | grep -v \"already\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After installing the dependencies you need to restart your kernel. The following cell does that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from helpers import get_data_loaders\n",
    "from helpers import seed_all\n",
    "from helpers import anomaly_detection_display\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure repeatibility\n",
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 48000 examples for training and 12000 for validation\n",
      "Using 10000 for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# This will get data loaders for the MNIST dataset for the train, validation\n",
    "# and test dataset\n",
    "data_loaders = get_data_loaders(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEgUlEQVR4nO2dSyh8YRjGzyCX3BYsyCUlZYEVK5eFhZRkJWLD3kJSZMPGrSxIyooNW41iwYZyKbG2wIbkWiI2lOa/mzxvOZgZ/znjeX6r83SO8331887bnPOdM75AIOAIDuKiPQHx/5BsIiSbCMkmQrKJkGwiEtx2+nw+fS+LMQKBgO+zfapsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJkKyiZBsIiSbCMkmwvV+Nit+vx9yXV0d5ObmZsh7e3u/PaWIoMomQrKJ8Lk9EfKXliVVVlYGt+vr62FfUlIS5P7+fshpaWmQn56eIDc0NEA+OjoKeZ7homVJwnEcyaZCson4sz27vb0d8uzsbHA7KysL9oX7JOv+/j7k2trasM4XDurZwnEcyaZCsomI2culCQk49dHRUch9fX2Q4+I+/79+f3+HPDQ0BLmgoAByT0/Pt+fpJVTZREg2EZJNRMz27M7OTsj2evZPGB4ehjw1NQV5YmLC9e/Ly8shFxYWQr64uAh5bpFElU2EZBMh2UTETM+urq6GPD09Ddnnw0vC19fXkNva2oLb4+PjsM9mi73WbcfKzMyEnJKS4nq+aKHKJkKyiZBsImKmZ3d3d0POyMiA/Pr6Crm3txfy7u5ucHt7exv2NTY2Qq6oqIBcVVUF2d7/vry8hPzw8OB4EVU2EZJNhGQTETM9+ytsn3x5efn02NzcXMjr6+thjX1+fg75/v4+rPP9FqpsIiSbCMkm4s/07JycHMirq6uQHx8fg9vZ2dmwL9x14x/P7WVU2URINhGSTUTM9OzT09MfHR8fHw/ZPt8VSVZWVn7t3JFElU2EZBMRMx/jNTU1kO3SoJ9g//b5+RmyXRo8MzMDuaWlBbJdtrS4uBjy3H4TVTYRkk2EZBPh2Z49MjICuampCfJXlzjtY7gHBwefHru0tATZPq5TVlbmOnZJSYnrXLyCKpsIySZCsonwTM9OT0+H3Nra6nr829sbZPvI7vHxMeStra1vz8WOnZ+f73r8wsLCt88dTVTZREg2EZJNhGd6tr3eXFpa6nr85OQk5Lm5uYjNpaOjA3JiYqLr8V69Fm5RZRMh2URINhGe6dn2+rPl9vYW8vz8fMTGTk5OhmxfbWVZW1uL2Nj/E1U2EZJNhGQT4ZmebdeF2Xx2dgb55uYm5LHsa7ZsD7avurLc3d2FPHY0UWUTIdlESDYRnunZdl2XzXl5eZCLi4sh259MtH13YGAguG3vV9vXbNmxr66uIH/82ahYQpVNhGQTIdlEeKZnf0VRURHkk5OTkM9lv8PbV2Ha9WwbGxuQ7Xf+WEGVTYRkEyHZRHjmJ5XtmrPDw0PIqampkMN5ndXOzg7ksbExyJubmyGfO9roJ5WF4ziSTYVkE+GZnm0ZHByE3NXVBfmrZ6KXl5chf3x9ld/vD2tuXkY9WziOI9lUePZjXISGPsaF4ziSTYVkEyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGSTYRkE+F6P1v8LVTZREg2EZJNhGQTIdlESDYR/wCAbA7fwgE9yQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(data_loaders['train'])\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig, sub = plt.subplots(figsize = (2,2)) \n",
    "sub.imshow(img, cmap='gray')\n",
    "_ = sub.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Autoencoder\n",
    "\n",
    "We'll train an autoencoder with these images by flattening them into vectors of length 784. The images from this dataset are already normalized such that the values are between 0 and 1. \n",
    "\n",
    "Here you will build a simple autoencoder. \n",
    "\n",
    "The encoder and decoder should be made of simple Multi-Layer Perceptrons. The units that connect the encoder and decoder will be the _compressed representation_ (also called _embedding_).\n",
    "\n",
    "Since the images are normalized between 0 and 1, you will need to use a **sigmoid activation on the output layer** to get values that match this input value range.\n",
    "\n",
    "For this exercise you are going to use a dimension for the embeddings of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        # Implement an MLP with an input size of 28*28, and an \n",
    "        # output layer of size encoding_dim\n",
    "        self.encoder = # YOUR CODE HERE\n",
    "        \n",
    "        ## decoder ##\n",
    "        # Implement an MLP with an input layer of size encoding_dim\n",
    "        # and an output layer of size 28*28\n",
    "        self.decoder = # YOUR CODE HERE\n",
    "        \n",
    "        self.auto_encoder = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten the input image\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        \n",
    "        encoded = self.auto_encoder(x)\n",
    "        \n",
    "        # Reshape the output as an image\n",
    "        # remember that the shape should be (batch_size, channel_count, height, width)\n",
    "        return encoded.reshape((x.shape[0], 1, 28, 28))\n",
    "    \n",
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Function\n",
    "\n",
    "As explained in the lesson, we can use the Mean Squared Error loss, which is called `MSELoss` in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to a normal training loop - however, this task is an unsupervised task, which means we do not need labels. The MNIST dataset does provide labels, of course, so we will just disregard them.\n",
    "\n",
    "For this simple autoencoder we do not need the GPU, so we will train on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model. Adjust as needed\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in tqdm(desc=\"Training\", total=len(data_loaders['train']), iterable=data_loaders['train']):\n",
    "        # we disregard the labels. We use the Python convention of calling\n",
    "        # an unused variable \"_\"\n",
    "        images, _ = data\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs.flatten(), images.flatten())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(desc=\"Validating\", total=len(data_loaders['valid']), iterable=data_loaders['valid']):\n",
    "            # _ stands in for labels, here\n",
    "            images, _ = data\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs.flatten(), images.flatten())\n",
    "            \n",
    "            # update running training loss\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # print avg training statistics\n",
    "    train_loss /= len(data_loaders['train'])\n",
    "    val_loss /= len(data_loaders['valid'])\n",
    "    print(\"Epoch: {} \\tTraining Loss: {:.6f}\\tValid Loss: {:.6f}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with your architecture until you get to a loss that is below 10. \n",
    "\n",
    "HINT: use at least two linear layers for the encoder and two for the decoder. Use ReLU activation for all layers except the output layer, where you need to use `nn.Sigmoid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Anomalies\n",
    "Now that our autoencoder is trained we can use it to find anomalies. Let's consider the test set. We loop over all the batches in the test set and we record the value of the loss for each example separately. The examples with the highest reconstruction loss are our anomalies. \n",
    "\n",
    "Indeed, if the reconstruction loss is high, that means that our trained autoencoder could not reconstruct them well. Indeed, what the autoencoder learned about our dataset during training is not enough to describe these examples, which means they are different than what the encoder has seen during training, i.e., they are anomalies (or at least they are the most uncharacteristic examples).\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this dataset is small we collect all the losses as well as\n",
    "# the image and its reconstruction in a dictionary. In case of a\n",
    "# larger dataset you might have to save on disk\n",
    "# (won't fit in memory)\n",
    "losses = {}\n",
    "\n",
    "# We need the loss by example (not by batch)\n",
    "loss_no_reduction = nn.MSELoss(reduction='none')\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(desc=\"Testing\", total=len(data_loaders['test']),\n",
    "            iterable=data_loaders['test']\n",
    "        ):\n",
    "\n",
    "            images, _ = data\n",
    "                        \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = loss_no_reduction(outputs, images)\n",
    "            \n",
    "            # Accumulate results per-example\n",
    "            for i, l in enumerate(loss.mean(dim=[1, 2, 3])):\n",
    "                losses[idx + i] = {\n",
    "                    'loss': float(l.cpu().numpy()),\n",
    "                    'image': images[i].numpy(),\n",
    "                    'reconstructed': outputs[i].numpy()\n",
    "                }\n",
    "            \n",
    "            idx += loss.shape[0]\n",
    "\n",
    "# Let's save our results in a pandas DataFrame\n",
    "df = pd.DataFrame(losses).T\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now display the histogram of the loss. The elements on the right (with the higher loss) are the most uncharacteristic examples. Feel free to look into `helpers.py` to see how these plots are made.\n",
    "\n",
    "> The output of the following cell is best viewed in Expand mode. To activate this mode, please click on Expand in the lower left of the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import anomaly_detection_display\n",
    "\n",
    "anomaly_detection_display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the bottom panels has the input in the first row and the reconstruction in the second row. \n",
    "\n",
    "Let's look at the first of the two panels. If your auto-decoder is sufficiently optimized, the most difficult numbers to reconstruct (the \"anomalies\" with the highest loss) will look pretty particular: they have some noise (like the vertical lines in some of the numbers), or are just not standard ways of drawing the respective numbers. As a result, the reconstructed images (second row) are not matching the inputs very well and the loss is high. These are anomalies.\n",
    "\n",
    "The second panel instead shows numbers taken from the peak of the distribution, and look indeed much more standard. The autoencoder can reconstruct them much better which results in a lower loss.\n",
    "\n",
    "In summary, the reconstruction loss can be used as a score proportional to how much a certain example is typical: a low loss means a typical example; a high loss means an atypical example, an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "We're dealing with images here, so we can (usually) get better performance using convolution layers. So, next we'll build a better autoencoder with convolutional layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
